 ===//===//===//===//===//===//>> Inicia a SparkSession
25/01/19 02:04:09 INFO SparkContext: Running Spark version 3.5.2
25/01/19 02:04:09 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/01/19 02:04:09 INFO SparkContext: Java version 17.0.12
25/01/19 02:04:09 INFO ResourceUtils: ==============================================================
25/01/19 02:04:09 INFO ResourceUtils: No custom resources configured for spark.driver.
25/01/19 02:04:09 INFO ResourceUtils: ==============================================================
25/01/19 02:04:09 INFO SparkContext: Submitted application: Filtragem de dados - 20250119_020409
25/01/19 02:04:09 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/01/19 02:04:09 INFO ResourceProfile: Limiting resource is cpu
25/01/19 02:04:09 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/01/19 02:04:09 INFO SecurityManager: Changing view acls to: ?,spark
25/01/19 02:04:09 INFO SecurityManager: Changing modify acls to: ?,spark
25/01/19 02:04:09 INFO SecurityManager: Changing view acls groups to:
25/01/19 02:04:09 INFO SecurityManager: Changing modify acls groups to:
25/01/19 02:04:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ?, spark; groups with view permissions: EMPTY; users with modify permissions: ?, spark; groups with modify permissions: EMPTY
25/01/19 02:04:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/01/19 02:04:09 INFO Utils: Successfully started service 'sparkDriver' on port 41245.
25/01/19 02:04:09 INFO SparkEnv: Registering MapOutputTracker
25/01/19 02:04:09 INFO SparkEnv: Registering BlockManagerMaster
25/01/19 02:04:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/01/19 02:04:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/01/19 02:04:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/01/19 02:04:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3164b9bb-d427-4f07-aaae-f7533f5b3647
25/01/19 02:04:09 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
25/01/19 02:04:09 INFO SparkEnv: Registering OutputCommitCoordinator
25/01/19 02:04:09 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/01/19 02:04:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/01/19 02:04:10 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/01/19 02:04:10 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 16 ms (0 ms spent in bootstraps)
25/01/19 02:04:10 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250119020410-0000
25/01/19 02:04:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40327.
25/01/19 02:04:10 INFO NettyBlockTransferService: Server created on spark-client:40327
25/01/19 02:04:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/01/19 02:04:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-client, 40327, None)
25/01/19 02:04:10 INFO BlockManagerMasterEndpoint: Registering block manager spark-client:40327 with 434.4 MiB RAM, BlockManagerId(driver, spark-client, 40327, None)
25/01/19 02:04:10 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250119020410-0000/0 on worker-20250119020339-172.18.0.3-8888 (172.18.0.3:8888) with 2 core(s)
25/01/19 02:04:10 INFO StandaloneSchedulerBackend: Granted executor ID app-20250119020410-0000/0 on hostPort 172.18.0.3:8888 with 2 core(s), 1024.0 MiB RAM
25/01/19 02:04:10 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250119020410-0000/1 on worker-20250119020339-172.18.0.4-8888 (172.18.0.4:8888) with 2 core(s)
25/01/19 02:04:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-client, 40327, None)
25/01/19 02:04:10 INFO StandaloneSchedulerBackend: Granted executor ID app-20250119020410-0000/1 on hostPort 172.18.0.4:8888 with 2 core(s), 1024.0 MiB RAM
25/01/19 02:04:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-client, 40327, None)
25/01/19 02:04:10 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250119020410-0000/1 is now RUNNING
25/01/19 02:04:10 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250119020410-0000/0 is now RUNNING
25/01/19 02:04:10 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
 ===//===//===//===//===//===//>> Cria um DataFrame simples com alguns dados
25/01/19 02:04:10 WARN FileSystem: Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.fs.viewfs.ViewFileSystem could not be instantiated
25/01/19 02:04:10 WARN FileSystem: org.apache.hadoop.security.KerberosAuthException: failure to login: javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input: name
        at jdk.security.auth/com.sun.security.auth.UnixPrincipal.<init>(UnixPrincipal.java:71)
        at jdk.security.auth/com.sun.security.auth.module.UnixLoginModule.login(UnixLoginModule.java:134)
        at java.base/javax.security.auth.login.LoginContext.invoke(LoginContext.java:755)
        at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:679)
        at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:677)
        at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
        at java.base/javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:677)
        at java.base/javax.security.auth.login.LoginContext.login(LoginContext.java:587)
        at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.login(UserGroupInformation.java:2065)
        at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1975)
        at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:719)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:669)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:579)
        at org.apache.hadoop.fs.viewfs.ViewFileSystem.<init>(ViewFileSystem.java:269)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
        at java.base/java.util.ServiceLoader$ProviderImpl.newInstance(ServiceLoader.java:789)
        at java.base/java.util.ServiceLoader$ProviderImpl.get(ServiceLoader.java:729)
        at java.base/java.util.ServiceLoader$3.next(ServiceLoader.java:1403)
        at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:3379)
        at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3424)
        at org.apache.hadoop.fs.FsUrlStreamHandlerFactory.<init>(FsUrlStreamHandlerFactory.java:77)
        at org.apache.spark.sql.internal.SharedState$.liftedTree2$1(SharedState.scala:199)
        at org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$setFsUrlStreamHandlerFactory(SharedState.scala:198)
        at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:54)
        at org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)
        at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)
        at org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)
        at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
        at java.base/java.lang.Thread.run(Thread.java:840)

25/01/19 02:04:10 WARN FileSystem: javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input: name
        at jdk.security.auth/com.sun.security.auth.UnixPrincipal.<init>(UnixPrincipal.java:71)
        at jdk.security.auth/com.sun.security.auth.module.UnixLoginModule.login(UnixLoginModule.java:134)
        at java.base/javax.security.auth.login.LoginContext.invoke(LoginContext.java:755)
        at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:679)
        at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:677)
        at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
        at java.base/javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:677)
        at java.base/javax.security.auth.login.LoginContext.login(LoginContext.java:587)
        at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.login(UserGroupInformation.java:2065)
        at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1975)
        at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:719)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:669)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:579)
        at org.apache.hadoop.fs.viewfs.ViewFileSystem.<init>(ViewFileSystem.java:269)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
        at java.base/java.util.ServiceLoader$ProviderImpl.newInstance(ServiceLoader.java:789)
        at java.base/java.util.ServiceLoader$ProviderImpl.get(ServiceLoader.java:729)
        at java.base/java.util.ServiceLoader$3.next(ServiceLoader.java:1403)
        at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:3379)
        at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3424)
        at org.apache.hadoop.fs.FsUrlStreamHandlerFactory.<init>(FsUrlStreamHandlerFactory.java:77)
        at org.apache.spark.sql.internal.SharedState$.liftedTree2$1(SharedState.scala:199)
        at org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$setFsUrlStreamHandlerFactory(SharedState.scala:198)
        at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:54)
        at org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)
        at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)
        at org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)
        at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
        at java.base/java.lang.Thread.run(Thread.java:840)

25/01/19 02:04:10 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/01/19 02:04:10 WARN SharedState: Cannot qualify the warehouse path, leaving it unqualified.
org.apache.hadoop.security.KerberosAuthException: failure to login: javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input: name
        at jdk.security.auth/com.sun.security.auth.UnixPrincipal.<init>(UnixPrincipal.java:71)
        at jdk.security.auth/com.sun.security.auth.module.UnixLoginModule.login(UnixLoginModule.java:134)
        at java.base/javax.security.auth.login.LoginContext.invoke(LoginContext.java:755)
        at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:679)
        at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:677)
        at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
        at java.base/javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:677)
        at java.base/javax.security.auth.login.LoginContext.login(LoginContext.java:587)
        at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.login(UserGroupInformation.java:2065)
        at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1975)
        at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:719)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:669)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:579)
        at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746)
        at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
        at org.apache.spark.sql.internal.SharedState$.qualifyWarehousePath(SharedState.scala:288)
        at org.apache.spark.sql.internal.SharedState.liftedTree1$1(SharedState.scala:80)
        at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:79)
        at org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)
        at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)
        at org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)
        at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
        at java.base/java.lang.Thread.run(Thread.java:840)

        at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1986)
        at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:719)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:669)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:579)
        at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746)
        at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
        at org.apache.spark.sql.internal.SharedState$.qualifyWarehousePath(SharedState.scala:288)
        at org.apache.spark.sql.internal.SharedState.liftedTree1$1(SharedState.scala:80)
        at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:79)
        at org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)
        at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)
        at org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)
        at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
        at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input: name
        at jdk.security.auth/com.sun.security.auth.UnixPrincipal.<init>(UnixPrincipal.java:71)
        at jdk.security.auth/com.sun.security.auth.module.UnixLoginModule.login(UnixLoginModule.java:134)
        at java.base/javax.security.auth.login.LoginContext.invoke(LoginContext.java:755)
        at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:679)
        at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:677)
        at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
        at java.base/javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:677)
        at java.base/javax.security.auth.login.LoginContext.login(LoginContext.java:587)
        at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.login(UserGroupInformation.java:2065)
        at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1975)
        at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:719)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:669)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:579)
        at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746)
        at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
        at org.apache.spark.sql.internal.SharedState$.qualifyWarehousePath(SharedState.scala:288)
        at org.apache.spark.sql.internal.SharedState.liftedTree1$1(SharedState.scala:80)
        at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:79)
        at org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)
        at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)
        at org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)
        at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
        at java.base/java.lang.Thread.run(Thread.java:840)

        at java.base/javax.security.auth.login.LoginContext.invoke(LoginContext.java:850)
        at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:679)
        at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:677)
        at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
        at java.base/javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:677)
        at java.base/javax.security.auth.login.LoginContext.login(LoginContext.java:587)
        at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.login(UserGroupInformation.java:2065)
        at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1975)
        ... 31 more
 ===//===//===//===//===//===//>> Filtra o DataFrame (idade menor que 23)
 ===//===//===//===//===//===//>> Salva o DataFrame filtrado em um arquivo de CSV
Traceback (most recent call last):
  File "/opt/bitnami/spark/conf/scripts-py/index_001.py", line 32, in <module>
    filtered_df.write.csv(output_path, header=True)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1864, in csv
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o52.csv.
: org.apache.hadoop.security.KerberosAuthException: failure to login: javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input: name
        at jdk.security.auth/com.sun.security.auth.UnixPrincipal.<init>(UnixPrincipal.java:71)
        at jdk.security.auth/com.sun.security.auth.module.UnixLoginModule.login(UnixLoginModule.java:134)
        at java.base/javax.security.auth.login.LoginContext.invoke(LoginContext.java:755)
        at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:679)
        at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:677)
        at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
        at java.base/javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:677)
        at java.base/javax.security.auth.login.LoginContext.login(LoginContext.java:587)
        at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.login(UserGroupInformation.java:2065)
        at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1975)
        at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:719)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:669)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:579)
        at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746)
        at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
        at org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:454)
        at org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:530)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
        at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
        at java.base/java.lang.Thread.run(Thread.java:840)

        at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1986)
        at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:719)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:669)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:579)
        at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746)
        at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
        at org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:454)
        at org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:530)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
        at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
        at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input: name
        at jdk.security.auth/com.sun.security.auth.UnixPrincipal.<init>(UnixPrincipal.java:71)
        at jdk.security.auth/com.sun.security.auth.module.UnixLoginModule.login(UnixLoginModule.java:134)
        at java.base/javax.security.auth.login.LoginContext.invoke(LoginContext.java:755)
        at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:679)
        at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:677)
        at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
        at java.base/javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:677)
        at java.base/javax.security.auth.login.LoginContext.login(LoginContext.java:587)
        at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.login(UserGroupInformation.java:2065)
        at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1975)
        at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:719)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:669)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:579)
        at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746)
        at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
        at org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:454)
        at org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:530)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
        at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
        at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
        at java.base/java.lang.Thread.run(Thread.java:840)

        at java.base/javax.security.auth.login.LoginContext.invoke(LoginContext.java:850)
        at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:679)
        at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:677)
        at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
        at java.base/javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:677)
        at java.base/javax.security.auth.login.LoginContext.login(LoginContext.java:587)
        at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.login(UserGroupInformation.java:2065)
        at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1975)
        ... 28 more

25/01/19 02:04:12 INFO SparkContext: Invoking stop() from shutdown hook
25/01/19 02:04:12 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:32892) with ID 0,  ResourceProfileId 0
25/01/19 02:04:12 INFO SparkContext: SparkContext is stopping with exitCode 0.
25/01/19 02:04:12 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:57342) with ID 1,  ResourceProfileId 0
25/01/19 02:04:12 INFO SparkUI: Stopped Spark web UI at http://spark-client:4040
25/01/19 02:04:12 INFO StandaloneSchedulerBackend: Shutting down all executors
25/01/19 02:04:12 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
25/01/19 02:04:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/01/19 02:04:12 INFO MemoryStore: MemoryStore cleared
25/01/19 02:04:12 INFO BlockManager: BlockManager stopped
25/01/19 02:04:12 INFO BlockManagerMaster: BlockManagerMaster stopped
25/01/19 02:04:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/01/19 02:04:12 INFO SparkContext: Successfully stopped SparkContext
25/01/19 02:04:12 INFO ShutdownHookManager: Shutdown hook called
25/01/19 02:04:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-be75f442-fd09-49e3-8f6d-b0a150865b2f
25/01/19 02:04:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-bd375fa2-32b6-4e7c-9799-3556528d4f11
25/01/19 02:04:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-bd375fa2-32b6-4e7c-9799-3556528d4f11/pyspark-493852fd-f314-41b8-8acb-26204321bc6f